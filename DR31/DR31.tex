\documentclass[a4paper,11pt,pdf]{pacmanreport}

%%=== Aditional packages
\usepackage{bm}
\usepackage{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} command for typesetting SI units
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{colortbl}
\usepackage{gensymb}
\usepackage{algorithm}
\usepackage{algorithmic}

% The following is used to make packages hyperref and cite work together
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[bookmarks=true,hyperfootnotes=true,colorlinks=true,linkcolor=blue,anchorcolor=blue,citecolor=blue,urlcolor=blue,filecolor=blue]{hyperref}

%%=== Local definitions
% Table float box with bottom caption, box width adjusted to content
\DeclareMathOperator{\rad}{rad}

\graphicspath{{images/}{../shared_images/}}

%% ================================
%% PROJECT INFO

\project{}
\projectid{FP7-IST-60918}
\projectstart{1 March 2013}
\duration{36}

%% ================================
%% DELIVERABLE INFO

\title{Control algorithms for haptic object exploration}
\deliverableid{DR 3.1}
\author{C. Rosales, M. Bonilla, G. Santaera, E. Luberto, M. Gabiccini, Jeremy L. Wyatt}
\address{Centro di Ricerca ``E. Piaggio'', Universit\`{a} di Pisa}
\email{carlos.rosales@for.unipi.it}
\headertitle{Haptic exploration}
\headerauthor{C. Rosales, G. Santaera, E. Luberto, M. Gabiccini, J. L. Wyatt}

\duedate{2015-02-28}
\submissiondate{2014-02-28}
\leadpartner{Universit\`{a} di Pisa}
\revision{draft}
\disseminationlevel{PU}

%% UNCOMMENT: to get the logo; if you've copied this file to a directory yearX/wpY/ then this should work
\reportlogo{pacmanlogo.png}

\begin{document}

\maketitle

\begin{abstract}
\noindent This report describes activities related to the development of haptic object exploration methodologies. Object shape refinement and friction coefficient estimation require low-level controllers that allow contour tracing interaction as well as a sophisticated exploratory probe. This report presents the efforts on providing controllers to perform sliding and rolling of a fingertip over a surface, integrating controller strategies with vision, implementing a robust testbed to simulate results, and last but not least, specifying the designs on the sensorization of the Pisa/IIT SoftHand.
\end{abstract}

\vspace{.2em}
\hrule

\footnotesize

\tableofcontents

\normalsize

\newpage

\section*{Executive Summary}
This report describes the activities within the PaCMan consortium to define methodologies for \emph{haptic object exploration}. The material included in this report shows the results of Task 3.1 (M 1-24). Aside, progress of Task 3.2 (M 18-36) regarding the envision of a promising approach to tackle the problem of information gathering for unknown objects. Developments of Task 3.3 (M 1-30) and its relation with WP4 is also reported.

\section*{Role of haptic object exploration in PaCMan}

This deliverable reports the research done on finding a methodology to explore objects by touch and vision, and gather their haptic properties such as the static and dynamic friction coefficient. We can state that the role of the haptic object exploration is to PaCMan what the friction coefficient is to grasp planning and execution: an essential element to be considered for success. The importance is equivalent but in a different arena, that is they refer to high and low level issues, respectively. In the project, several sources of uncertainties are to be considered for grasping, and those includes the object pose and detection estimation, as well as its mechanical properties estimation. A common representation is to be found within the project to aid performing robust grasps, and this deliverable provides means to gather the complementary tactile information to feed and be integrated with visual information to feed object representations.
%To this end, \emph{control algorithms} and \emph{innovative sensors} are developed.

\section*{Contribution to the PaCMan scenario}

The multi-modal object representation to be reported in WP2 requires information about the shape and haptic properties of an object.
The active acquisition of such information is essential to build the representation of a particular object. The exploratory strategies which combine low-level control algorithms developed by the UNIPI team, high-level decision making strategies to be completed by UoB team, together with a sensorized adaptive hand as proposed from the UNIPI team, make this possible.

Additionally, the developed algorithms make no assumption on how the gathered information is encoded. This provides a flexible testbed to contrast the representation coming from WP2 with approaches external to the project.

\newpage

\section{Tasks, objectives, results}

\subsection{Planned work}

This report must show the results of Task 3.1. Particularly, it should describe reactive control strategies for haptic exploration of an object by a robotic hand, consisting of contour tracing of the object surface and finger rolling over the object surface, as well as strategies for extracting higher-order geometric features and frictional properties. 

% In Task 3.3 we planned to implement the active gaze model we presented in year 1 in the robot platform. This was intended to be used in the demonstration 5.2. The hypothesis to be tested is whether a reward based framework for active gaze control for grasping under positional uncertainty can be extended to work on real objects. The initial assumption was that the active gaze control model would work with a prior model of the object shape. The overall goal was to show that active gaze can improve the reliability of grasping and manipulation.

\subsection{Actual work performed}

\subsubsection{Task 3.1}

We can proudly say that Task 3.1 has been accomplished. The results from this task crystallized in~\cite{Rosales2014Active} which is appended in Sec.~\ref{sec:slidingPointClouds}. Details on how the planned activities were followed are described in MS 4.2.

In previous approaches, in general, the friction coefficient is neglected during acquisition when being in contact with the object. For instance,~\cite{Ilonen2013Fusing} fuses the two sources of information, visual and tactile, by applying symmetry constraints to reconstruct the complete object shape, and~\cite{Chalon2013Online} uses the contact points to improve the object pose within the hand, initially obtained by vision. Nonetheless, other object properties apart from the shape are hardly explored, perhaps due to the fact that one needs specific sensory systems to measure them and a proper representation to process the information. In our work, the hardware setup resembles that of~\cite{Bajcsy1984What}, but instead of using tactile arrays at the tip of the exploratory probe, we use an intrinsic tactile sensor based on~\cite{Serio2014Tactile} due to of its capability to retrieve the contact point and force~\cite{Bicchi1993Contact}, which is crucial to our approach. Thus, in the paper we propose a way to estimate the friction coefficient from an object that has been previously detected from vision. The visual information is encoded using a Gaussian Process~\cite{Rasmussen2006Gaussian}. This representation is exploited to generate trajectories over the object surface, which then is explored by means of sliding controllers implemented within the project (see MS 4.2 and Sec~\ref{sec:slidingPointClouds}). The friction coefficient is also encoded using a Gaussian Process using both visual and tactile shape measurements. This provides a common probabilistic framework for both representations, however, we plan to replace such codification with the multi-modal representation reported on DR 2.1 during year 3.

% To be prepared by: Carlos
% Learning the haptic characteristics of objects by exploration in-hand
% \input{./inputFiles/LearningHapticCharacteristicsObjectsExplorationInHand.tex}

\subsubsection{Task 3.2}

Previous work on object detection assisted by touch typically comprise fully actuated hands, such as the work presented in~\cite{Bimbo2013Combining}. To this end, the initial estimate provided by the visual input must be accurate enough in order to place precisely the finger areas covered with tactile arrays over the object surface, or use proper compliant controllers~(see MS 4.2, Sec.~A.2). This implies solving the object recognition problem as well. Taking the extreme case where no visual input is available, the remarkable work by~\cite{Petrovskaya2011Global} propose a Bayesian approach termed Scaling Series, however, the testbed for imformation gathering was a robot arm equipped with a single force/torque sensor which allow to compute with high accuracy the contact point w.r.t. to the robot base, similarly to the one reported in the previous task. The inconvenient of such setup fall in the probing time. As the authors of~\cite{Petrovskaya2011Global} say, with that setup (recall they don't use vision input at all in contrast to our work~\cite{Rosales2014Active}), each probe took approximately $10$s, and in the meantime, the algorithm, which is not light, was allowed to do computations. They reported that at least $3$ probes were required in order to get reasonable low uncertainty in the estimate, which sum up to $30$s to have a good estimate of the object pose and detection.

The fact that the object shape and localization uncertainty is reduced by making contact, combined with the adaptability of the Pisa/IIT SoftHand to any object shape, influenced the idea of using the hand as sophisticated exploratory probe w.r.t. previous testbeds. The configuration is similar to the usual adopted in bio-mechanic systems~\cite{Cappozzo1995}, where direct sensorization of the limbs is advised via optical or magnetic markers, or inertial devices. In addition, such sensorization should ``strap down'' to the greatest extent to the underlying bone, while defining the simplest assembly of technical joints that best reproduces the animal or human joint motion~\cite{Lucchetti1998}. Thus, we propose a glove-inspired inertial-based solution to read the hand configuration for object detection as the result of a grasping action. We strongly believe that the proposed solution will be of great help in situations where partial or none visual information is available, as challenged in Task 3.2. For this reason, we dedicated efforts to continue on this research as it will beneficial to accomplish the project goals with an alternative methodology to the one the initially proposed. The following two paragraphs show the preliminary and on-going positive results on this topic, respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% To be prepared by: Marco Gabiccini
% Low-cost, Fast and Accurate Reconstruction of Robotic and Human Postures
% via IMU Measurements
\input{./inputFiles/ReconstructionPosturesImuMeasurements.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% To be prepared by: Gaspare and Emanuele
% Endowing the Pisa/IIT SoftHand with the sense of touch
%\input{./inputFiles/SoftHandWithSenseOfTouch.tex}
\paragraph{Endowing the Pisa/IIT SoftHand with the sense of touch}

We successfully extended the proposed solution above to the case of the five-finger Pisa/IIT SoftHand. We validated the approach by visual inspection as shown in Fig.~\ref{fig:hand_reconstruction_1}. Additional, we suggest watching the video sequences for static \href{https://www.youtube.com/watch?v=0oVha0Q1vWM}{static} and a \href{https://www.youtube.com/watch?v=bceOXa990-Q}{dynamic} configurations\footnote[1]{External links are readily identified in the PDF and might not be visible in a black\&white hard-copy, please, click on the PDF to go to the on-line site.}.

\begin{figure}
\centering
\mbox{
\includegraphics[width=0.33\linewidth]{Hand_Movement_1.png}
\includegraphics[width=0.33\linewidth]{Hand_Movement_2.png}
\includegraphics[width=0.33\linewidth]{Hand_Movement_3.png}
}
\caption{Hand posture reconstruction examples}
\label{fig:hand_reconstruction_1}
\end{figure}

We used a naive recognizer to test the proposed solution, and we were able to discriminate similar cups in shape but in different sizes as shown in Fig. \ref{fig:Object_1}. For more in-sight on the recognition examples, we suggest to see videos by clicking on \href{https://www.youtube.com/watch?v=d_WPQ3WmHRg}{Object 1}, \href{https://www.youtube.com/watch?v=PG38VObdl6o}{Object 2}, \href{https://www.youtube.com/watch?v=bIYhLXm90hc}{Object $3_a$}, \href{https://www.youtube.com/watch?v=IXVlBAoGKho}{Object $3_b$}, \href{https://www.youtube.com/watch?v=Efmm6-JHcxU}{Object $4_a$}, \href{https://www.youtube.com/watch?v=NZElSV_AnJ4}{Object $4_b$}, \href{https://www.youtube.com/watch?v=mDDb5oTaHzM}{Object $5_a$} and \href{https://www.youtube.com/watch?v=sLzU39zffFY}{Object $5_b$}. For more details on the implementation, refer to Sec.~\ref{sec:imuSoftHand}.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{Object_1.png}
\includegraphics[width=0.8\linewidth]{Object_3.png}
\includegraphics[width=0.8\linewidth]{Object_5.png}
\caption{Object recognition examples using the same cup of different sizes.}
\label{fig:Object_1}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Task 3.3}

Concerning Task 3.3, we must say the the work on grasping of novel objects in WP4 progressed much faster than expected. The active gaze approach taken has thus been informed by this progress. Specifically the fact that experiments in our IJRR submission for WP4 has shown us that grasp failure is typically driven by the incompleteness of the point cloud near to suggested grasp points and along the final grasp trajectory. In addition, tests with differing numbers of views of the test object prior to grasping showed that the more views one has from an object, the greater the probability of grasping success for this object. In addition, we have used a wrist mounted depth camera for the active gaze method. We are extending the reward-based method from~\cite{nunez2013models} to the case of incomplete point clouds. We require a measure of the proportion of the total possible coverage given by the incomplete object model. To this end, we are currently investigating the use of an octree representation, called octomap~\cite{hornung13auro}, for online object modelling and sensor data fusion. The octomap of a given object allows the representation of occupied (known voxels belonging to a segmented object), free (voxels that do not belong to an object), and unknown areas of the scene, which might belong to the object if they are near known and occupied voxels. We are now implementing a method to estimate the safety of a grasp trajectory given this octomap. We are also testing different measures of the coverage of the object pertinent to a candidate grasp given the point cloud and the octomap representations.

Relevant work on grasping under incomplete information is that by Bohg and Kragic \cite{bohg:icra11}. There the approach is not active gaze to fill in information, but to use a symmetry prior to complete missing object parts. An active vision system for grasping is described in \cite{gratal:irosws10} \comment{[CR] Not found}, but in this the main goal of gaze control is to find and fixate on the object to support a visual servoing routine. There is relatively little published work that deals with active vision specifically in the service of manipulation. A more general but related problem is the use of active vision to recover the pose and complete shape of an object. Recently Krainin et al. \cite{Kra11Aut} devised a next-best-view algorithm that autonomously acquires a complete shape model of an unknown object. The next-best-view is obtained by iterating over a grid of feasible viewpoints and selecting the (relative) camera pose with the highest score adjusted for the cost of rotating the object to the new viewpoint. The work also considers the volume occupied by the robotic manipulator (the configuration of which is known), to segment the object from the robot, and to plan new grasps in order to reveal object parts that were occluded by the manipulator. This is the closest to the approach we are currently working on. Finally A recent workshop at RSS 2014 on active information gathering for grasping was notable in that none of the papers considered active vision, but instead focussed on the type of approaches we study Task 4.4.


%In year 1, we presented an algorithm for active gaze in the case where the object model is known, and the pose is uncertain. This was tested in simulation, and shown to fit human data. Relevant work on grasping under incomplete information is that by Bohg and Kragic \cite{bohg:icra11}. There the approach is not active gaze to fill in information, but to use a symmetry prior to complete missing object parts. An active vision system for grasping is described in \cite{gratal:irosws10}, but in this 
%The main goal of gaze control is to find and fixate on the object to support a visual servoing routine. A recent workshop at RSS 2014 on active information gathering for grasping was notable in that none of the papers considered active vision, but instead focussed on the type of approaches we study Task 4.4. Thus the area is underexplored.

% \subsection{Relation to the state-of-the-art}
% How are the obtained results related to the state-of-the-art?
% This part is usually discussed in the corresponding subsection. Therefore, % a global 'Relation to the state-of-the-art' is unnecessary

% To be prepared by: Jeremy Wyatt
% Active Gaze
% \input{./inputFiles/ActiveGaze.tex}
\newpage
\bibliographystyle{IEEEtran}
\bibliography{../shared_bibliography/abbreviations,./bibliography/DR31}

\newpage
\appendix
\section{Annexes}
\label{sec:annex}

% Which papers / articles are included in the report? Mention titles, authors, publication info; abstract; and a one-liner relating the publication back to the discussion on actual work performed.

\subsection{Article: Active Gathering of Frictional Properties from Objects} \label{sec:slidingPointClouds}
\begin{description}
	\item[Authors] Carlos Rosales, Arash Ajoudani, Marco Gabiccini, and Antonio Bicchi
	\item[Info] Published, see~\cite{Rosales2014Active}.
	\item[Abstract] This work proposes a representation that comprises both shape and friction, as well as the exploration strategy to gather them from an object. The representation is developed under a common probabilistic framework, particularly it uses a Gaussian Process to approximate the distribution of the friction coefficient over the surface, also represented as a Gaussian Process. The surface model is exploited to compute straight lines (geodesic flows) that guide the exploration. The exploration follows these flows by employing an impedance
controller in pursuance of safety, shape accommodation and contact enforcement, while measuring the necessary data to estimate the friction coefficient. The exploratory probes consist of an RGBD camera and an Intrinsic Tactile sensor (ITs) mounted on a robotic arm. Experimental results give evidence for the effectiveness of the algorithm in the friction coefficient gathering and enrichment of the object representation.
	\item[Relation with the deliverable] The article succesfully integrate vision with the developed controller for the sliding primitve to refine the object shape and estimate the friction coefficient (static and dynamic), and concludes Task 3.1.
	\item[Attachment] (following pages until next annex)
\end{description}
\includepdf[pages=-]{./attachedPapers/ActiveGatheringOfFrictionalPropertiesFromObjects.pdf}

\subsection{Article: Low-cost, Fast and Accurate Reconstruction of Robotic and Human Postures via IMU Measurements} \label{sec:imu2Fingers}
\begin{description}
	\item[Authors] Gaspare Santaera, Emanuele Luberto, Alessandro Serio, Marco Gabiccini, and Antonio Bicchi.
	\item[Info] Accepted for publication in Proc. of the 2015 IEEE Int. Conf. on Robotics and Automation
	\item[Abstract] In this paper, we present a method to reconstruct the configurations of kinematic trees of rigid bodies not using
measurements of relative angles (such as, e.g. rotary encoders at joints) but absolute posture sensors (such as IMUs) along with suitable filter algorithms. We argue that the relatively larger inaccuracies shown by absolute sensors can be compensated by suitable processing, such as a passive complementary filters exploiting the Mahony-Hamel formulation. The proposed method is applicable to  systems where measurements of relative angles is not feasible or convenient, or where the joint kinematics are not lower pairs: for example, human body parts or soft robotic devices. In the paper, we make explicit reference to the reconstruction of posture of the compliant, underactuated Pisa/IIT SoftHand. Quantitative comparisons with ground truth data in grasping tests are used to validate the proposed method. The resulting hardware design is mechanically robust, cheap and can be easily adapted to robotic hands with different structures, as well as to sensorizing gloves for studying human grasping strategies.
	\item[Relation with the deliverable] The proposed solution in this article is the preliminary result for the next annex.
	\item[Attachment] (following pages until next annex)
\end{description}
\includepdf[pages=-]{./attachedPapers/ReconstructionPosturesImuMeasurements.pdf}

\subsection{Technical report: Endowing the Pisa/IIT SoftHand with the sense of touch} \label{sec:imuSoftHand}
\begin{description}
	\item[Authors] Gaspare Santaera, Emanuele Luberto, Marco Gabiccini, and Antonio Bicchi.
	\item[Info] Internal report, in preparation for submission to a conference.
	\item[Abstract] A modified version of the Mahony-Hamel passive complementary filter is used to obtain the orientation of a frame $\{ A \}$ with respect to another frame $\{ B \}$, expressed by a rotation matrix and subject to kinematic constraints, to reconstruct a two-finger gripper. We extend that work to the five-finger Pisa/IIT SoftHand and the potential application of object recognition without vision and using a naive recognizer.
	\item[Relation with the deliverable] The proposed solution in this report aims to tackle the problem of object exploration and recognition in the case where partial or none visual information is available. This scenario is challenged in Task 3.2.
	\item[Attachment] (following pages)
\end{description}
\includepdf[pages=-]{./attachedPapers/imuSoftHand.pdf}


\end{document}
